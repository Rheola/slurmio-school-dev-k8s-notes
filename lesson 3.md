---
date created: 2021-10-07 19:00:18 (+03:00), Thursday
---
# Урок 3: Абстракции приложения. Вечерняя школа «Kubernetes для разработчиков» [youtube](https://www.youtube.com/watch?v=LLVfC08UVqY)

- Разбираемся с replicaset и deployment
- Бонусом Resources: как правильно работать с ресурсами кластера

# Ответы на вопросы из чата:
- 1
    - Q: Кто такой Developer Advocate, кого от кого он защищает, чем занимается
    - A: Это публичное лицо, которое представляет комьюнити для компании и наоборот. Посредник между продуктами компании и пользователями, комьюнити. Защищает разработчиков, инженеров и всех прочих людей в IT от незнания, это такое связующее звено между комьюнити какой-то технологии и компанией, группой разработчиков, которые эту технологию предоставляют. В данном случае Павел выступает посредником между тем, что он с коллегами делает внутри своей компании (MSC), теми продуктами, которые выходят на рынок и комьюнити пользователей этих продуктов. Одна из его задач — доносить способы пользоваться продуктами так, чтобы было хороши и пользователям и компании-провайдеру, чтобы ни у кого по ночам ничего не горело. С точки зрения комьюнити задача Developer Advocate — доносить то, что думает комьюнити относительно продукта до product owner'ов, директоров, конечных разработчиков и т.д., всех людей в команде продукта.
- 2
    - Q: По поводу сказанного о портах, что директивы в YAML файле являются фактически документацией
    - A: Аналогии на стримах могут упрощать реальную картину мира. В примере с портами было сказано, что если указывать в yaml файле пода порты — то это не более чем документирование и ни на что более не влияет. На самом деле кое на что влияет, если зайти на под и выполнить там команду env, можно увидеть большое количество прокинутых переменных окружения, если описывать в поде этот порт, то kubernetes будет генерить и добавлять для других подов соответствующую переменную окружения. Это сделано для сервис дискавери, но в реальности этим практически никто не пользуется. Основная мысль — могут быть опущены детали, которые сильно усложнят понимание для новичков за счёт потока ненужной на первоначальном этапе информации. При этом ведущие стараются не врать, а говорить исходя из сложившихся практик, например, в данном случае, бОльшей правдой является, что описание порта нужно для документирования, а не то, что приложение будет работать с ним. Это один из примеров, такие упрощения еще будут встречаться

По поводу споров насчёт того, можно ли под рассматривать как процесс, скорее контейнер можно рассматривать как процесс, а под — как неделимый инстанс приложения, а кубер это ОС, которая всем этим рулит, да это тоже упрощение, метафора

В прошлый раз мы остановились на том, что запустили под с приложением, узнали что если нам нужно 2 экземпляра приложения, то нам придется отредактировать файл с описанием пода, переименовать под, т.к. в одном пространстве имён не может быть 2х одинаковых сущностей одного типа, и непосредственно создать его из этого файла, пришли к выводу, что это совсем не удобно, а в kubernetes есть более удобные способы масштабировать приложения, чем поды.

# ReplicaSet ![презентация](./assets/lesson_3/3.Application_abstractions_02.png)
- Это объект, который представляет собой набор реплик приложения
- В общем и целом внутри yaml описания ReplicaSet содержится темплейт описания подов, которые будут создаваться
- При создании подов будут проставляться специальные метки, ярлыки (label) на поды, по которым ReplicaSet будет определять, что это именно его поды
- У самого ReplicaSet есть информация о селекторе, т.е. он знает, его поды это те, которые удовлетворяют этому селектору
- Содержит поле, указывающее, сколько мы хотим создать подов из нашего темплейта

# Практика
- git must have, если есть пробелы — можно пройти [бесплатный курс от слёрма](https://slurm.io/git)
- работаем с каталогом `~/school-dev-k8s/practice/2.application-abstractions/2.replicaset`
- `replicaset.yaml` выглядит так:

```yaml
---
# file: practice/2.application-abstractions/2.replicaset/replicaset.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: my-replicaset
spec:
  replicas: 2
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - image: quay.io/testing-farm/nginx:1.12
        name: nginx
        ports:
        - containerPort: 80
...
```
- у ReplicaSet, так же как и у пода, так же как и у всего в kubernetes структура описания это yaml
- как и у всех объектов в kubernetes у него есть тип — `kind` и версия api (поднималость в первом уроке)
- обращаем внимание, что у пода версия апи была v1, у ReplicaSet стабильная версия это apps/v1
- тип, как ни странно, ReplicaSet
- информацию о типах и всякое такое можно узнать в документации, на сайте https://kubernetes.io/
- далее будет лайфак, где брать нужную инфу по kubernetes прямо из консоли
- так же как у пода, у ReplicaSet есть metadata, там мы указываем имя
- аналогично, spec — специализация нашего объекта
- отличительные относительно пода параметры ReplicaSet:
    - replicas — количество реплик нашего приложения, подов с одним и тем же описанием, мы хотим запустить
    - селектор (selector), в данном описании видно, что мы выбираем все поды с меткой `app: my-app`
    - темплейт (template)
        - содержит описание пода, при этом имя пода не указывается, т.к. оно будет генерироваться автоматически
        - в метаданных содержит label, соответствующий селектору
        - spec
            - такой же, как в описании пода
            - имена контейнеров нужно указывать, в отличие от имён подов в темплейтах, причём имена в рамках пода должны быть уникальны
            - описание портов — для документации (но не совсем, как было сказано выше)

# По темплейтам
- pod — это абстракция, которая хоть и является минимальной единицей, с которой умеет работать kubernetes, но в реальности мы почти никогда не создаём поды напрямую, потому что это неудобно. Можно считать pod полуслужебной абстракцией kubernetes.
- Т.к. мы не хотим работать с каждым подом отдельно, мы хотим описать темплейт пода и дальше сказать kubernetes с помощью ресурса ReplicaSet "создай N подов"

# По лейблам
- Они могут быть присвоены вообще любому объекту, например, мы можем присвоить метку и самому ReplicaSet
- Почему лейблы это хорошо
    - kubernetes сам их использует, например, чтобы понимать, какие поды привязаны к каким ReplicaSet
    - по ним можно понимать, что за приложение, мы можем навешать кастомных человекочитаемых меток и по ним ориентироваться
    - инструменты для работы с kubernetes, тот же `kubectl get`, позволяют фильтровать, например, `kubectl get pod -l "app=myapp"` выдаст поды с соответствующей меткой
- Рекомендуется везде проставлять и использовать лейблы, особенно в рамках курса, далее будут достаточно сложные, приближенные к реальности, конфигурации, там везде будут лейблы

# Возвращаемся к консоли
чистим за собой с прошлого раза:
```shell
kubectl delete pod --all # удаляем все поды
kubectl delete replicaset --all # удаляем все репликасеты
```
\<sarcasm>лайфхак для разработчиков с рут доступом к проду, которые хотят порадовать своих коллег из эксплуатации:
```shell
kubectl delete all --all -A # пройтись по всем неймспейсам и удалить все объекты со всеми именами, аналог rm -rf
```
\</sarcasm>
# Разница между create и apply
- `create` — команда, которая выполняется только 1 раз, второй раз выдаст ошибку
- `apply`
    - применяет изменения, идемпотентна, т.е. создаст, то чего нет, если всё уже есть и соответствует описанию — ничего не сделает, если есть расхождения в описании и реальности, исправит их
    - можно указывать для аргумента `-f` не только файл, но и директорию
    - обычно используется в пайплайнах CI/CD


создаём (аплаим) репликасет:
```shell
kubectl apply -f replicaset.yaml
```
запросим информацию по ReplicaSet, используем сокращение `rs`:
```shell
kubectl get rs
```
получим:
```shell
NAME            DESIRED   CURRENT   READY   AGE
my-replicaset   2         2         2       2m25s
```
- DESIRED — сколько мы указывали создавать реплик
- CURRENT — сколько по факту есть
- READY — цифра 2 показывает что 2 пода сейчас работают и доступны. Что конкретно это значит, будет разбираться позже, т.к. затрагивает сетевые абстракции

Проверяем поды:
```shell
kubectl get po
```
выдаст нам что-то вроде:
```shell
NAME                  READY   STATUS              RESTARTS   AGE
my-replicaset-g8lc4   0/1     ContainerCreating   0          88s
my-replicaset-n7bxt   0/1     ContainerCreating   0          88s
```
- замечаем, что поды называются по имени ReplicaSet + некий кусок хэша, это сделано во избежание попыток создания одноимённых однотипных сущностей
если мы посмотрим на детальную информацию по одному из подов:
```shell
k describe pod my-replicaset-g8lc4 # если не указать имя пода, то получим информацию о всех подах
```
то сможем увидеть строку `Labels:       app=my-app`
можем запросить информацию о подах с указанием этой метки:
```shell
kubectl get po -l "app=my-app"
```
вывод будет аналогичен тому что был выше, т.к. других подов пока нет
# Про self-healing
- Репликасет призван поддерживать нужное количество реплик, независимо от внешних обстоятельств, пока явно не указано другое, например, если уборщица, выгнав вас из-за стола, случайно введёт команду удаления пода или одна из нод кластера выйдет из строя, на свободных ресурсах будет выполнена попытка запустить недостающий под в репликасете.


Например, удаляем под и с небольшой задержкой снова запрашиваем информацию:
```shell
kubectl delete pod my-replicaset-g8lc4
kubectl get po -l "app=my-app"
```
получаем
```shell
NAME                  READY   STATUS    RESTARTS   AGE
my-replicaset-hnwjp   1/1     Running   0          65s # это уже новый под
my-replicaset-n7bxt   1/1     Running   0          16m
```

# Про скейлинг (масштабирование):
- 1 способ — открыть наш yaml файл и исправить там количество реплик и выполнить аплай изменений
- 2 способ — kubectl scale. Данным способом можно как добавлять, так и убирать целевое количество реплик
- При скейлинге вниз одним из ключевых критериев является возраст пода, первыми будут удаляться самые молодые поды, т.е. созданные позже

Пример со вторым способом:
```shell
kubectl scale --replicas 3 rs my-replicaset
kubectl get po -l "app=my-app"
```
получим такую картину:
```shell
NAME                  READY   STATUS              RESTARTS   AGE
my-replicaset-hnwjp   1/1     Running             0          12m
my-replicaset-n6v8r   0/1     ContainerCreating   0          13s
my-replicaset-n7bxt   1/1     Running             0          28m
```
В кластере есть ограничения, 4 пода на неймспейс (пользователя?)
# Про автокомплит
- Павел рекомендует настроить автокомплит для `kubectl` и тем самым облегчить себе жизнь
- Для `zshell` есть отдельный плагин `kubectl`
- Для чистого bash есть `kubectl completion`, можно вывести справку командой kubectl completion -h, там всё описано

# Экспериментируем с подом
## Пример 1
- проверяем теорию о том, что кластер автоматически прибьет под, созданный руками, но с такой же меткой, как в ReplicaSet
- берём конфиг из предыдущего урока и добавляем туда:
```yaml
---
# file: practice/2.application-abstractions/1.pod/pod.yaml
apiVersion: v1
kind: pod
metadata:
  name: my-pod
  labels:
    app: my-app # добавили метку как в ReplicaSet
spec:
  containers:
  - image: quay.io/testing-farm/nginx:1.12
    name: nginx
    ports:
    - containerPort: 80
...
```
применяем, проверяем:
```shell
k apply -f pod.yaml
k get po
```
видим:
```shell
NAME                  READY   STATUS        RESTARTS   AGE
my-pod                0/1     Terminating   0          4s  # видим, что свежесозданный под удаляется
my-replicaset-hnwjp   1/1     Running       0          55m
my-replicaset-n7bxt   1/1     Running       0          70m
```
- Теория работает, ReplicaSet, который отслеживает какие поды его и сколько их, по метке `my-app`, увидел что в этом неймспейсе появился ещё один под с таким же лейблом, он посчитал его лишним и прибил, это нормальное явление.
- Обычно в реальности именно так не делают, но может произойти так, что подов в репликасете будет больше чем ожидалось, например, при выходе из строя ноды кластер через какое-то время перестанет считать приложения на этой ноде работающими, даже если они фактически работают, например, если нода упала не полностью, а отказали только некоторые компоненты. Кластер для себя пометит такие поды как недоступные и создаст новые, на доступных нодах, согласно описанию ReplicaSet. По возвращении ноды в строй может так сложиться, что мы получим больше подов чем указано в описании ReplicaSet, тогда подам на вернувшейся ноде будет отправлен сигнал на уничтожение

## Пример 2
- Обновление приложения
- Допустим, мы собрали новый докер образ и хотим заменить им старый
- Мы можем увидеть в описании теплейта для ReplicaSet, что в названии образа (пути к образу?) содержится тег с версией, в нашем случае 1.12, допустим, мы хотим обновиться до 1.13
- Самый очевидный способ — поправить файл с описанием и выполнить apply, делать так мы конечно не будем
- Другой способ — использовать команду ниже:
 ```shell
 kubectl set image replicaset my-replicaset nginx=quay.io/testing-farm/nginx:1.13
 ```
 - В шаблоне пода может быть несколько контейнеров, поэтому мы должны указать, для какого именно мы меняем образ
 - Не забываем, что учебном кластере не стоит работать с докерхабом, т.к. это исчерпает лимиты на подключения, поэтому используем другие репозитории, в данном случае quay.io
 - Т.к. в данном случае у нас всего один контейнер, мы можем сократить команду, это часто используется в простых пайплайнах:
 ```shell
 kubectl set image replicaset my-replicaset '*=quay.io/testing-farm/nginx:1.13'
 ```
 - Если после этого выполнить kubectl describe rs, увидим, что указана новая версия образа
 - Если мы выполним kubectl describe po, увидим что контейнеры живы уже давно и версия образа указана старая. Так происходит, потому что командами выше мы меняем темплейт, а задача ReplicaSet — отслеживать количество подов по метке. Для реального обновления версии в поде мы можем начать прибивать существующие поды, тогда новые будут созданы с версией образа из шаблона
 - Вывод — ReplicaSet не решает задачу обновления приложения но...

# Deployment ![презентация](./assets/lesson_3/3.Application_abstractions_03.png)
- Это более высокоуровневая абстракция над ReplicaSet
- Позволяет решать проблему обновления приложения
- Когда мы создаём Deployment, он создаёт под собой ReplicaSet, а он создаёт под собой поды в нужном количестве
- Если мы меняем версию образа в Deployment, он создаёт новый ReplicaSet и с помощью Rolling Update или стратегии Recreate передеплоивается и у нас появляется приложение новой версии в нашем кластере kubernetes

## Очень сложно, ничего непонятно, идём в консоль
чистим за собой:
```shell
kubectl delete replicaset --all # удаляем все репликасеты
```
- При удалении верхнеуровневой абстракции все относящиеся к ней объекты абстракций более низкого уровня удаляются автоматически

меняем директорию:
```shell
cd ../3.deployment/
```

## Q&A
- |
    - Q: Какое кресло у спикера?
    - A: Expert Star GR https://www.falto.ru/catalogue/expert-star.php
- |
    - Q: Можно ли указывать количество реплик при скейлинге через арифметические вычисления, наприер `kubectl scale --replicas +3 rs my-replicaset`
    - A: kubectl такое не умеет, средствами bash и т.п. традиционно можно (сначала получаем через get, обрабатываем и формируем нужную команду)
- |
    - Q: Что будет при скейлинге вниз с работой, выполняемой подом, завершится, прервётся, будет выполнять другой под
    - A: Ответить на вопрос нам поможет kubectl explain
        - например, kubectl explain pod выдаст информацию о всех верхнеуровневых полях для pod
        - отвечая на вопрос
            - нужно заглянуть в spec пода, выполняем kubectl explain pod.spec
            - находим описание параметра terminationGracePeriodSeconds и читаем
            - это поле можно указать в подах, шаблонах ReplicaSet или Deployment, оно говорит о следующем:
                - когда завершается приложение в контейнере, приложению отправляется sigterm, т.е. мы просим его корректно (gracefully) завершиться.
                    - terminationGracePeriodSeconds указывает, сколько секунд даётся на эти операции, по умолчанию 30 секунд
                    - Если приложение так умеет, то оно принимает sigterm, обрабатывает оставшиеся запросы или другую работу и после этого самостоятельно корректно завершается
                    - Если приложение не умеет обрабатывать sigterm, то оно завершается сразу же, все обрабатываемые им запросы прерываются, есть ряд техник для обработки таких ситуаций (pre-stop хуки, возможно об этом будет позже)
                    - Если приложение на sigterm не реагирует, то через заданное в terminationGracePeriodSeconds время ему будет отправлена команда sigkill, т.е. оно будет мгновенно завершено
- |
    - Q: Шелл у Павла (в чате есть ответы)
    - A: https://github.com/pauljamm/dotfiles

## Возвращаемся к консоли и работаем с Deployment
- заглянем в yaml файл в нашей директории, видим следующее:
```yaml
---
# file: practice/2.application-abstractions/3.deployment/deployment.yaml
apiVersion: apps/v1 # видим что версия API такая же как у ReplicaSet
kind: Deployment # Соответствующий нашей теме тип
metadata:
  name: my-deployment
spec: # описание идентичное описанию предыдущего ReplicaSet
  replicas: 2
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - image: quay.io/testing-farm/nginx:1.12
        name: nginx
        ports:
        - containerPort: 80
...
```
- почти то же что и ReplicaSet, но есть отличия в поведении

Применяем наше описание и смотрим что получилось:
```shell
kubectl apply -f deployment.yaml
k get deploy
```
видим:
```shell
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
my-deployment   2/2     2            2           81s
```
смотрим поды:
```shell
k get po
```
видим:
```shell
NAME                             READY   STATUS    RESTARTS   AGE
my-deployment-79788cc48d-bnqr7   1/1     Running   0          2m32s
my-deployment-79788cc48d-qfnkp   1/1     Running   0          2m31s
```
Видим особенности отличий именования подов ReplicaSet от подов Deployment

если  посмотрим теперь на ReplicaSetы, то увидим
```shell
NAME                       DESIRED   CURRENT   READY   AGE
my-deployment-79788cc48d   2         2         2       5m13s
```
- Итого, Deployment создал нам ReplicaSet, сгенерировал ему имя на базе имени деплоймента и создал 2 пода в этом ReplicaSet, везде добавляя кусочки хэшей
- Хэши считаются от содержимого того, что мы применяем, возможно зависит ещё от кластера, тут Павел не уверен, можно поискать информацию отдельно
- Совет из практики  не нужно помогать kubernetes вручную, например, у нас есть Deployment, не нужно пытаться скейлить ReplicaSet или добавлять/удалять поды напрямую, с бОльшей вероятностью это навредит а не поможет, лучше искать корневую причину проблем, которые заставляют вас делать такое, а kubernetes сам справится со своими задачами

## Обновляем приложение в рамках Deployment
- Так же как в ReplicaSet, обновляем образ в Deployment одним из способов:
    - через правку yaml и apply
    - командой `kubectl set image deployment my-deployment '*=quay.io/testing-farm/nginx:1.13'`
    - новый способ — редактирование ресурса на лету с помощью kubectl edit, например:

```shell
kubectl edit deployment my-deployment
```
- данная команда вызовет редактор по умолчанию (в unix-like системах задаваемый с помощью переменной окружения EDITOR) и позволит редактировать именно объект в кластере, т.е. мы просим кластер прислать нам полное описание указанного деплоймента, который вот прям щас в кластере запущен, далее, кластер нам его прислал, kubectl открыл нам его в редакторе и когда мы внесём и сохраним изменения, эти изменения попадут не в какой-то локальный файл, а сразу отправится обратно в кластер kubernetes и там применится
- это удобный способ быстро экспериментировать, дебажить, выстрелить себе в ногу, и т.п., но совсем не хороший способ работать с production кластером, особенно, если вы работаете с ним не один, а с коллегами, т.к. данные изменения трудно отследить
- по хорошему, нужно работать через yaml файлы описаний, git, apply через CI/CD, желательно еще тестами обвязать и вот это вот всё

- при работе через edit мы видим множество полей, которые мы сами не создавали, это все доступные поля для данного объекта и в них указаны значения по умолчанию для данного кластера
- после изменения версии образа nginx опросив поды мы видим похожую картину:
```shell
NAME                             READY   STATUS              RESTARTS   AGE
my-deployment-79788cc48d-bnqr7   1/1     Running             0          29m
my-deployment-79788cc48d-qfnkp   0/1     Terminating         0          29m
my-deployment-d7fcf87f9-vztvm    1/1     Running             0          19s
my-deployment-d7fcf87f9-zk4tq    0/1     ContainerCreating   0          3s
```
- т.е. старые поды удаляются, а новые создаются, по идентификаторам можно догадаться, что у нас поменялся ReplicaSet
- если запросим информацию по ReplicaSet, то увидим:
```shell
NAME                       DESIRED   CURRENT   READY   AGE
my-deployment-79788cc48d   0         0         0       33m  # старый ReplicaSet
my-deployment-d7fcf87f9    2         2         2       3m53s
```
- Далее Павел вручную делает и показывает, что обычно происходит "под капотом", при изменениях в Deployment:
    - Создаёт первый ReplicaSet, где указана старая версия образа, с ожидаемым количеством подов равным 2
    - Проверяет, что у нас есть 2 пода, принадлежащих первому ReplicaSet
    - Создаёт второй ReplicaSet с новой версией образа, с ожидаемым количеством подов равным 0
    - Проверяет, что у нас есть 2 ReplicaSet, и первому принадлежит 2 пода, а второму 0
    - Далее выполняет постепенный скейлинг обоих реплик, в нашем примере с шагом 1, т.е. у нас будет 2/0, затем 1/1, затем 0/2 подов со старой и новой версиями образа соответственно
    - Проверяет, что в итоге у нас всё как ожидалось — есть 2 ReplicaSet и уже второму принадлежит 2 пода с новой версией приложения, а первый ReplicaSet остался без подов

## Зачем kubernetes делает обновление с использованием ReplicaSet'ов и почему старый ReplicaSet не удаляется?
- Для демонстрации снова создаём Deployment из yaml файла, затем правим в Deployment (не в файле) версию образа на более новую, проверяем что произошло обновление и мы имеем 2 ReplicaSet, старый и текущий, с обновленной версией приложения
- Собственно, старый ReplicaSet нужен, чтобы иметь возможность выполнить откат, это можно сделать командой:
```shell
kubectl rollout undo deployment my-deployment
```
- Таким образом мы производим скейлинг репликасетов в обратном направлении
- Когда мы работали с Deployment через edit, мы там могли увидеть такой параметр как revisionHistoryLimit, также мы можем почитать о нем с помощью команды `kubectl explain deployment.spec.revisionHistoryLimit`
    - Данный параметр по умолчанию равен 10, он отвечает за глубину хранения ReplicaSet'ов для наших Deployment'ов, то есть, количество версий, на которые мы сможем откатиться
- откатить откат можно той же командой, таким образом мы переключимся снова на новую версию приложения
- убедиться, что каждый создаётся новая ревизия, можно командой `kubectl rollout history deployment my-deployment`, если мы проделаем откат несколько раз, то увидим увеличивающиеся значения в выводе

## Как kubernetes решает, каким именно образом скейлить ReplicaSet'ы?
- Если мы заглянем в `kubectl explain deployment.spec`, то сможем там увидеть параметр **strategy**
- Смотрим подробнее — `kubectl explain deployment.spec.strategy` и видим, что существует поле type, которое может принимать 2 значения — "Recreate" или "RollingUpdate", по умолчанию используется RollingUpdate
    - RollingUpdate мы уже видели в действии, это тот случай, когда реплики обновляются постепенно, без даунтайма. Это применимо для приложений, которые поддерживают обратную совместимость, что позволит существовать одновременно двум версиям приложений
        - Для данной стратегии можно настроить еще 2 поля, оба принимают значения в процентах или целых числах:
            - maxSurge (от слова всплеск) — говорит о том, на сколько штук или процентов мы можем поднять количество подов при обновлении, относительно их желаемого количества, т.е. значения spec.replicas в нашем yaml описании
                - Например, у нас указано replicas: 2 и maxSurge: 1, таким образом в момент обновления у нас может быть 3 пода
            - maxUnavailable — говорит о том, насколько можно опустить количество реплик нашего приложения относительно желаемого значения
                - Например, у нас указано replicas: 2 и maxUnavailable: 1, таким образом в момент начала обновления 1 реплику сразу можно удалить
            - В случае, когда у нас оба значения равны 1 и replicas = 2, в момент начала обновления происходит примерно следующее:
                - уничтожается 1 старая реплика, в этот же момент создаются 2 новых реплики
                - kubernetes ждет, пока 2 новых реплики досоздаются, станут готовы (в дальнейших лекциях об этом будет подробнее)
                - уничтожается оставшаяся старая реплика
            - Так не принято, но в некоторых случаях приходится поддерживать приложения, которые существуют в 1 экземпляре, т.е. `replicas = 1`
                - Для таких приложений параметры RollingUpdate по умолчанию не подойдут, т.к. мы столкнемся с временной недоступностью приложения.
                - Чтобы избежать такой проблемы, необходимо установить maxSurge в 1, а `maxUnavailable` в 0, тогда сначала будет добавлена новая реплика и после её готовности будет удалена старая
            - Когда мы работаем с большим или динамическим количеством реплик, предпочтительней задавать данные параметры в виде процентов, по умолчанию оба параметра установлены в 25%
    - Recreate - стратегия, при которой сначала удаляются все старые поды, затем создаются новые. Основной минус данной стратегии — она неизбежно приводит к даунтайму, т.к. в какой-то момент старых подов уже не будет, а новых еще не будет

# Namespace
- Оно же пространство имён, это способ логически разделить кластер kubernetes на части
- Мы можем разделить наш кластер на отдельные пространства имён
- Помимо просто логического разделения, на неймспейсах еще много чего построено в kubernetes, в частности, с помощью специальных политик можно настроить, сможем ли мы или нет, взаимодействовать с подами из соседних неймспейсов
    - Бывают политики доступа, сетевые политики, с помощью сетевых политик, например, можно настроить, сможем ли мы сходить из одного неймспейса по сети в другой, на какой порт какого приложения и т.д.
- В рамках одного Namespace нельзя создавать объекты одного типа с одинаковыми именами, в рамках разных NaNamespace — можно
- Что делают с помощью Namespace'ов
    - Организовывают совместное использование кластера, например:
        - Наш учебный кластер — каждый пользователь работает в своём неймспейсе
        - Можно разделять кластер по командам разработки, это активно применяется в [MCS](https://mcs.mail.ru/)
    - Можно делить кластер по неймспейсу на приложение
    - Можно делить кластер по окружениям — dev/test/stage/prod
- Некоторые ресурсы бывают "неймспейсные", т.е. создаются и существуют в рамках неймспейса, например, pod, deployment, из того с чем еще поработаем — configmap, PVC, ingress, service
- Некоторые ресурсы создаются в кластере, так называемые cluster wide, т.е. они общие для всего кластера (StorageClass, persistent volumes, ClusterRoleBinding, ClusterRole и т.д., это будет разбираться дальше)


# Resources
- У подов в кластере kubernetes есть 2 основных ресурса, это память и CPU
- Мы можем указывать, сколько нашему приложению для работы нужно выдать ресурсов в кластере kubernetes
- Выдача ресурсов настраивается двумя параметрами ![презентация](./assets/lesson_3/3.Application_abstractions_05.png):
    - Limits
        - Это верхняя граница количество ресурсов, которое pod и наше приложение в нём (контейнер точнее?) может максимально использовать 
            - Например, если разрешено использовать 2 CPU, то приложение больше не получит, а в случае, если приложение попытается выделить больше указанного объёма памяти, придёт [OOM killer](https://neo4j.com/developer/kb/linux-out-of-memory-killer/), убъет контейнер, соответственно, убъет старый и создаст новый pod
                - Был случай, когда из-за данной особенности работы kubernetes приложение на Java, которое текло по памяти, проработало несколько недель, пока это не заметили, т.е. инстансы приложения перезапускались в разное время и глобально всё продолжало работать
    - Requests
        - Количество ресурсов, которое резервируется для пода на ноде

## Как это всё работает
- У нод есть capacity по ресурсам, рассмотрим пример с памятью и ядрами CPU
- Kubernetes принимает решения, куда отправить под на базе capacity нод и значений в requests
- Например у наших нод capacity 16 CPU / 16 GB
- Если мы создадим под, у которого в requests будет указано `cpu: 8` и этот под будет развёрнут на одной из таких нод, то свободная ёмкость по CPU у данной ноды станет 16-8=8, если таких подов будет 2, то на данную ноду больше не смогут попасть поды с заданными requests
- Важно понимать, что requests не имеют никакого отношения к реальному потреблению ресурсов подами, то есть, мы можем указать в requests пода `cpu: 2`, 2 CPU будет зарезервировано на ноде, из её capacity эти 2 CPU будут отняты, но это значит, что наш под действительно будет нагружать 2 ядра, возможно они будут работать вхолостую

## Разбираем файл с описанием deployment для лучшего понимания 
- переходим в каталог `~/school-dev-k8s/practice/2.application-abstractions/4.resources` содержащий интересующий нас файл
- открываем его и видим:

```yaml
---
# file: practice/2.application-abstractions/4.resources/deployment-with-resources.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - image: quay.io/testing-farm/nginx:1.12
        name: nginx
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 10m
            memory: 100Mi
          limits:
            cpu: 100m
            memory: 100Mi
...
```
- нас интересует блок resources и параметры внутри
- объемы памяти указываются, судя по всему, в [мебибайтах](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D0%B1%D0%B8%D0%B1%D0%B0%D0%B9%D1%82), то есть, в привычных всем айтишникам мегабайтах, где для получения значения мы пользуемся степенями двойки
- суффикс "m" в параметре cpu означает millicpu, 1/1000, т.е. 100m = 0.1 CPU
- если указывать cpu в целых значениях, без суффикса, то это будет означать целые CPU
- это упрощённое представление, но оно покрывает 100% наших нужд на данный момент

## QoS Class
- если мы применим наш файл и с помощью describe выведем информацию об одном из созданных подов, мы сможем увидеть  поле `QoS Class: Burstable`
- QoS — это сокращение от quality of service, соответственно QoS Class => класс обслуживания
- QoS классы в kubernetes тоже относятся к ресурсам, к лимитам и реквестам и их сочетаниям
- От класса QoS зависит то, как kubernetes будет обрабатывать нехватку ресурсов:
    - 1. Best Effort
        - Когда мы не ставим на наше приложение никаких лимитов и реквестов, оно может приехать на любую ноду кластера и использовать все доступные ресурсы, но если ресурсов не будет хватать, то приложение сможет получить только те ресурсы, которые свободны
        - Например, если на ноде началась нехватка ресурсов и там есть поды с QoS классом Best Effort, то это поды будут удалены с ноды в первую очередь (и пересозданы где-то еще), чтобы освободить ресурсы для оставшихся подов с остальными QoS классами, чтобы они могли продолжать работать
    - 2. Burstable
        - Когда у нас указаны реквесты, но не указаны лимиты, либо лимиты больше чем реквесты
        - Наш файл описания деплоймента выше это демонстрирует — мы запрашиваем у ноды 10 millicpu, но можем потреблять до 100 millicpu
        - Поды данного класса во 2ю очередь будут удаляться с нод, испытывающих проблемы с ресурсами и пересоздаваться где-то еще
    - 3. Guaranteed, наивысший QoS класс
        - Тот случай, когда указанные лимиты и реквесты равны
        - Механизм удаления подов с QoS классами ниже сделан для того, чтобы поды с QoS данного класса работали как можно дольше на своих нодах
        - Для важных приложений имеет смысл использовать данный класс QoS

## Продолжаем экспериментировать с ресурсами
- Еще один способ изменить наш деплоймент на лету, актуально для разного рода автоматизации, в данном случае мы увеличиваем лимиты ресурсов:
```shell
kubectl patch deployment my-deployment --patch '{"spec":{"template":{"spec":{"containers":[{"name":"nginx","resources":{"requests":{"cpu":"100"},"limits":{"cpu":"100"}}}]}}}}'
```
- После применения патча у Павла в списке подов появился один со статусом **Pending**, это означает, что kubernetes не может найти для пода подходящую ноду, удовлетворяющую по ресурсам, указанным в деплойменте
- Посмотреть подробности данной проблемы можно с помощью describe, мы сможем увидеть что-то подобное (в учебном кластере проявляется по другому, см. ниже):
![FailedScheduling.png](./assets/lesson_3/FailedScheduling.png)
- Читаем так: нет ни одной ноды, которая может удовлетворить реквестам в 100 CPU
- Это достаточно частая в реальности ситуация, когда мы пытаемся деплоить, а ресурсов кластера не хватает
    - Это может говорить о том, что у приложения слишком большие требования и нужно с этим что-то сделать, либо дествительно есть проблемы с выделенными ресурсами в кластере и это задачка для отдела эксплуатации
- В учебном кластере проявляется иначе. После применения патча, проверяем `kubectl describe deployments.apps my-deployment`. Видим, что создается ReplicaSet, но новых подов в выводе нет
```shell
OldReplicaSets:    my-deployment-6d6f49d794 (2/2 replicas created)
NewReplicaSet:     my-deployment-7ccb8f8966 (0/1 replicas created)
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  5m55s  deployment-controller  Scaled up replica set my-deployment-6d6f49d794 to 2
  Normal  ScalingReplicaSet  5m51s  deployment-controller  Scaled up replica set my-deployment-7ccb8f8966 to 1
```
Проверим, что выдает `kubectl get rs` и `kubectl describe rs`. Видим, что вторая реплика создалась не полностью из-за политики безопасности
```shell
>kubectl get rs

NAME                       DESIRED   CURRENT   READY   AGE
my-deployment-6d6f49d794   2         2         2       11m
my-deployment-7ccb8f8966   1         0         0       11m

>kubectl describe rs my-deployment-7ccb8f8966

  Warning  FailedCreate  11m               replicaset-controller  Error creating: pods "my-deployment-7ccb8f8966-2wdlg" is forbidden: exceeded quota: resource-quota, requested: limits.cpu=2,requests.cpu=2, used: limits.cpu=200m,requests.cpu=20m, limited: limits.cpu=500m,requests.cpu=500m
```

# Q&A
- |
    - Q: Что будет если я запущу двухпоточное приложение по одновременной сортировке двух массивов в каждом потоке и выдам ему меньше одного ядра (100 millicpu)
    - A: Ответ будет рассмотрен в теме про особенности конкретных языков программирования
- |
    - Q: Когда приложение стартует в поде видит ли оно сколько было прописано в реквестах или лимитах?
    - A: Также, ответ будет рассмотрен в теме про особенности конкретных языков программирования
        - Если кратко, зависит от того, куда приложение смотрит... Также зависит от языка, Java современная видит, Java 7 не видела
- |
    - Q: Что почитать о том, как поднимать с 0 кластер kubernetes
    - A: 
        - У Слёрма была еще одна бесплатная школа, больше админской направленности. Там было рассказано про kubespray. [Записи на youtube](https://www.youtube.com/watch?v=Jp866ltZBSk&list=PL8D2P0ruohOA4Y9LQoTttfSgsRwUGWpu6)
        - Можно отдельно поискать информацию про kubespray, kubeadm
        - Чтобы прям погрузиться — https://github.com/kelseyhightower/kubernetes-the-hard-way. Автор тоже developer advocate, он создал инструкцию о том как поднимать кластер kubernetes, без всяких вспомогательных инструментов, прям руками выписывая все сертификаты, скачивая и запуская бинарники и т.д. Очень хорошо прочищает мозги на предмет того как это работает, но это если вы прям хотите погрузиться
- |
    - Q: Почему kubernetes перешел с docker-контейнеров на какие-то другие
    - A:  
        - Текущее руководство компании Docker Inc стали проприетизировать продукт, внедрять лимиты, платные версии и т.д.
        - Docker — это довольно большая система, которая умеет собирать контейнеры, запускать, управлять ими с помощью swarm, в общем, это довольно большой engine, а чем больше продукт, тем больше точек отказа. А сам docker у себя под капотом использует контейнерную технологию, технологию для запуска самих контейнеров, под названием runC. Поэтому в какой-то момент ребята из комьюнити kubernetes подумали, что им не нужен весь docker, взяли за основу runC и на его основе написали своё минималистичное решение для запуска контейнеров, потому что kubernetes не требуется собирать эти контейнеры, не нужна прочая куча функционала docker. Получились всякие штуки типа crio, containerd, project Moby, которые умеют запускать контейнеры максимально легковесно. Все эти штуки называются CRI - container runtime interface. Все эти штуки и сам docker CRI-совместимые, поэтому всё что было создано в docker, работает с любым CRI.
        - Если говорить о дальнейшей судьбе docker — он не умрёт, это инструмент разработчика, ops инженера, инструмент CI/CD — собрать контейнер, запустить локально, docker-compose — прекрасный инструмент и т.п.
- |
    - Q: Что такое реплика?
    - A: Копия приложения, N реплик = N копий приложения
- |
    - Q: Как управлять несколькими кластерами kubernetes одновременно, есть ли best practice, чтобы не путаться и не стрелять себе в ногу
    - A:
        - Речь о том что у вас локально 2 конфига разных кластеров?
        - Когда я работал со stage и prod кластерами, я сделал алиас prodctl, чтобы не путаться, он подключал конфиг от продакшн кластера. Но пару раз ошибки всё же были
        - Сейчас у меня везде сконфигурирован кусочек prompt, который показывает, какой контекст сейчас используется, я привык проверять контекст перед применением изменений
        - Для случаев, когда совсем страшно что-то сломать, для доступа к prod среде сделать отдельный jump host, adminbox — отдельную виртуалку, через которую будет происходить вся работа
- |
    - Q: Есть ли в kubernetes стратегия деплоймента blue/green, чтобы трафик шёл только на одну версию приложения в один момент?
    - A: Встроенных нет, возможно, это есть в рамках этого или предыдущего курса, в любом случае, можно найти на эту тему материалы в сети, делается непринужденно
- |
    - Q: Как меньше уставать на работе? Возможно у меня выгорание или возраст берёт своё...
    - A: Ответ Павла: Обратиться к психологу, это прям правильно и полезно для здоровья
        - Дополнение от меня: Стоит обратить внимание на материалы Максима Дорофеева, они могут помочь либо улучшить ситуацию, либо убедиться, что нужно обращаться к специалистам
- |
    - Q: Как что округляется, когда значения заданы в процентах
    - A: Коллеги, так же как в прошлый раз, рекомендую поэкспериментировать
- |
    - Q: Есть ли где-то бесплатный/пробный managed kubernetes
    - A: 
        - MCS (mail.ru), если вы не майнер и не спамер, вам могут начислить бонусы и вы сможете получить пробный период
        - Наверняка, существуют другие провайдеры с такой услугой, можно найти в гугле
- |
    - Q: По поводу книг
    - A:
        - Книги успевают устаревать быстрее чем выходят, поэтому самое стоящее — документация, https://kubernetes.io. Там и статьи в блогах есть и инструкции для начинающих и много чего еще
        - Также, есть интерактивные курсы https://www.katacoda.com/, там есть kubernetes

# Из чата:
- [Programmatically generated handy kubectl aliases.](https://github.com/ahmetb/kubectl-aliases)
